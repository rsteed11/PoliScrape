//Mentorship Notes//

National Security Archive @ George Washington U
* nsarchive.gwu.edu *
	- Declassified US foreign policy documents
	- Subscription-side, UNC log-in
	- UNREDACTED blog: discussions on politics of declassification

- Electronic briefing books - organized by region and nothing else (no search function)
- Blog attempts to provide "single reference for all FOIA inquiries" with the use of hyperlinks
- Server error when tried to access actual NSA


State Department's Office of the Historian  
* history.state.gov/historicaldocuments/ *
	- _Foreign Relations of the United States_: 
	- only Truman through early Reagan
	- Also, historical documents (conversations, etc.)

- search function allows filters to certain parts of the website
- simple string returns tons of results
- also allows Booleans, wildcards (? and *), nesting


University of Wisconsin Digital Library _Foreign Relations of the US_
* uwdc.library.wisc.edu/collections/FRUS/ *
	- From series inception, 1861-1960
	- Search parameters: strings, compound strings, proximity searches (second or third term within x words of first); perhaps the only repository with these features?	

- Not geared around FOIA; based on making more library material accessible on internet
- * for truncation
- Searches types of works (like OH filter)
- Boolean search (AND, OR, NOT) with nesting (only once though)
- Change number of results
- Can do proximity with Booleans (in same sentence or paragraph)
- Proximity is within x number of words
- Can do "near," "followed by," or opposite of both


8-25

TODO:

//- Find book IBN//
//- Look into, take notes on web crawling - try to start something?//
//- Visit Undergrad Library to look into textbook//

LEARNED: 
- Google works so fast because it saves search results on server (not possible for us)
- Jumping from site to site is relatively easy, but figuring out how to extract information from FRUS files is tougher
- How to parse results so understandable?

8-30

TODO:

//- Adapted search terms, returns text around search and formats text better. -//
//- Trawled through Wisconsin, identified simple terms and brought back sample text. -//


9-1

TODO:

//- Doesn't recognize some links?//
- How to deal with PDFs: http://digicoll.library.wisc.edu/cgi-bin/FRUS/FRUS-idx?type=article&did=FRUS.FRUS1862v01.i0002&id=FRUS.FRUS1862v01&isize=M 
//- Robots.txt?//
- Work from html.

LEARNED:

- No PID... non-onyen Sakai login?
- At Wisconsin, PDFs are scans - no text to analyze, but for the title.
- All three websites used for testing do not have robots.txt files.
- To do this from HTML
	* Use php to exec(some unix command)
	* Get result -> a file -> the html page

9-6

TODO:

- For Dr. McKeown
	* Demo web crawler proto - features:
		- Visits web pages.
		- Checks page status - 200 visited and available, 404 not found
		- Searches for search term provided by converting page text to string, lowercase.
		- If not found, collects searchable URLs from page and adds them to "toVisit" list - visits until term is found
		- Stops at defined limit.
		- Provides sample of text around search term, once found.
	* Features to add:
		- Visit robots.txt file and use that to search -- can't happen, none of the target links have a sitemap.
		- Compile all search results and don't stop at first (harder)
		- Put the search results in a more useful and comprehensible form - provide link to source.
		- Deal with visual PDFs...
	* Biggest challenges: 
		- Refining search so that the crawler doesn't waste time - Office of the Historian great because all html, but bad because of lots of links!
		- i.e., refine search locations - would be useful to have sitemap
		- Saving a variety of searches in a server, to reduce search times
		- Learn to use php within webpages - should be useful to have iwphys development environment

LEARNED: 

- Madison contact, when ready
- Look through web crawler information to become more efficient
- Deal with PDFs
	* theory: used to be in two formats, PDF and text
	* use test strings that get small number of hits - contained in paper
	* Error analysis
- Crack FRUS (economically), incrementally add target sites 
- Solve Onyen problem, Sakai connection

9-8

TODO:

- Investigate Lemur and similar text-parsing search tools
- Look for proxy services that will expedite coding process
- Record sources, current methodologies for Mentorship assignment

LEARNED:

- Installed lemur library, quite the process
	- Lemur parses XML, HTML, PDF, etc. and streamlines search engines
	- Would need to use Web Parser
	- Not sure how this works when connecting to websites... seems more like you have a file and search it from terminal?
- Scrapy
	- In python
	- Added new example crawler

9-13 

TODO:

- Look into getting that textbook
- Investigate results of Scrapy web crawler
- Figure out what's going on in Sakai...
- Investigate other proxy services at bookmarked page

LEARNED:

- Building working Scrapy proto

9-15

TODO:

- Use Scrapyd to manage crawler
- Investigate other proxies
- Textbook form
- Can't seem to get anything to print to page...
				






