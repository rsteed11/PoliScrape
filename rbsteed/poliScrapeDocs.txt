Know your users: what will be referenced?
What are they trying to do?
Address each function.

When I want to use PoliScrape, I will:


Home

PoliScrape is an open source web crawler built using Scrapy, an open source web crawling library. This documentation contains basic PoliScrape functions and references.

The Scrapy documentation is below:

http://scrapydocs

Both the crawler and the website are available on GitHub at the following address. GitHub version control is highly recommended for project changes or merges. 

** github project **
** github version control tutorial **

The crawler also uses the BeautifulSoup project, located here:

** BeautifulSoup docs **

The crawler also utilizes regular expressions, a module whose documentation is located here:

** re docs **

This documentation covers the following topics:

** Topic Hierarchy Here **

Any further issues can be submitted here:

** github issues **


---------------------
Running the Crawler

Currently, there are two crawlers included in the PoliScrape project. The first, named "history," targets the State Department's Office of the Historian FRUS collection. The second, called "wisconsin," targets Wisconsin's FRUS collection. To run either crawler, navigate to the project home directory, <PoliScrape/poliScrapy/foreignScrape>. In the command line, run:
	
	scrapy crawl crawlerName

To adjust the first targeted site or volume for a crawler, open the crawler script, located in <PoliScrape/poliScrapy/foreignScrape/spiders/spiderName.py>. Change or append to the start_urls variable. The allowed_domains variable restricts the base domain searched, and prevents the crawler from straying from the scraping site. Multiple allowed_domains are accepted.

** class definition **

Crawlers scrape specific items from the webpage. These items are defined in <PoliScrape/poliScrapy/foreignScrape/items.py>. Any new items must be instantiated here. To define a new item or adjust an existing one, simply redefine it in the crawler script using the variable item['itemName']. Here, BeautifulSoup is used to parse body text, while simple HTML response parsers are used for other items.

Currently, both crawlers are calibrated to exclude extraneous webpage data. To adjust crawler scraping scope, PoliScrape utilizes an in-script filter with a simple "for" loop and "if" statement. URLs that do not contain a key word such as "frus1945" are excluded from the crawler's queue. This keyword can be replaced, or additional keywords added.

** parse function **

---------------------
Settings

Pipeline Settings

Once items are scraped (see Items), they are shuttled to an item pipeline, <PoliScrape/poliScrapy/foreignScrape/pipelines.py>. Here, items are exported to a single file entitled <spiderName_products.xml>. Each item is formatted into an XML (Extensible Markup Language) value, and appended to a single <item> tag. Each scraped page is appended to this document, creating a single, large file for each crawler run. To change this file name, simply adjust the pipeline settings.

** pipelines.py ** 


Feed Settings

Additionally, an XML file is created using an exporter which contains the XML documents outputted by the item pipeline. Its name can be adjusted in the crawler settings file, <PoliScrape/poliScrapy/foreignScrape/settings.py>. The feed exports files to the <PoliScrape/poliScrapy/xmlItems> directory, where runs are organized by day scraped and named by time scraped.

** item pipelines settings **


Web Courtesy

The settings.py file also defines bot name and other web crawling behaviors. By default, PoliScrape adheres to robots.txt files to prevent server overloading and subsequent IP address banishment. The bot is limited to 32 concurrent requests by default, and throttled to a 2 second download delay. These settings should always be maintained.

** courtesy settings **


Other Settings

PoliScrape is highly customizable. PoliScrape outputs a log file each run, along with a closeSpider report. These are defined as extensions, which can be removed or added in the settings.py file.

** extensions **

There are a series of sentinel failsafes that halt spider progress if triggered. By default, the spider will halt after 30 thrown errors. It may also halt after a finite number of visited pages is reached, a time limit is met, or an item count is surpassed. These latter three are disabled by default, but can be activated or adjusted in the settings.py file.

** close spider **

The depth priority setting prioritizes pages immediately below the start URL in the web hierarchy. For more depth priority options, visit the Scrapy documentation.

** depth priority **

To create a new crawler, use the Scrapy documentation. The project name for new crawler instantiation is contained in settings.py.

---------------------
Using the Virtual Computing Laboratory

While the PoliScrape application can be operated from any Linux or Unix server, including local machines, it is recommended that users operate PoliScrape using UNC's Virtual Computing Laboratory. To use the laboratory, follow the instructions for reserving a machine available on UNC IT's site:

** UNC IT VCL tutorial **

A full blade Ubuntu disk image environment containing Scrapy and BeautifulSoup is available on the VCL reservation page, entitled "Scrapy, Ubuntu 14.04 LTS Svr (Full Blade)". Using a remote shell and GitHub's cloning method, download the PoliScrape repository to a remote machine.

** remote shell UNC tutorial **
** GitHub git clone method **

Once the PoliScrape application is initialized, run the crawler (see "Run the Crawler"). VCL sessions are finite and all data, including the PoliScrape application, will be wiped after the time reservation is over. Scraped data must be transferred to local or remote storage before the timeout.

File Transfer

PoliScrape data is stored in Longleaf mass storage at the University of North Carolina at Chapel Hill.Shell into onyen@longleaf.its.unc.edu and navigate to </ms/depts/polisci/PoliScrape/>. Run the secure copy file transfer protocol from the VCL shell program to transfer a scraped file, exercising caution and creating file copies using the <cp originalFile newFile> command to avoid loss of data: 

** scp scrapedFile onyen@longleaf.its.unc.edu:/ms/depts/polisci/PoliScrape/xmlItems **

To transfer a directory recursively (best method for maintaining date and time hierarchical organization from "Feed Settings"):

** scp -r scrapedDir onyen@longleaf.its.unc.edu:/ms/depts/polisci/PoliScrape/xmlItems **

File Storage

Conventionally, files are stored in Longleaf at </ms/depts/polisci/PoliScrape/xmlItems/. There is a "history" directory for raw, XML, scraped data and two other directories for split items from each crawler (see "File Splitting").

---------------------
File Splitting

PoliScrape includes a Python XML file splitting script to parse crawler run files. Usually run in the Longleaf Linux cluster (see "File Transfer"), the xmlSplitter script (<PoliScrape/poliScrapy/xmlSplitter.py) iterates through each webpage item stored in the XML crawl file.Each file is copied into a separate file and named by its "id" item, or URL. A hierarchical tree file structure resembling the original web hierarchy is constructed as files are stored using their respective HTML URI paths. 


There is a separate xmlSplitter file for each crawler, given varying keywords. To run the xmlSplitter file, use the following command:

** python xmlSplitter-crawlerName.py scrapedFile storageDir **

The scraped file will be split into its requisite components and a hierarchy constructed in the specified storage directory in Longleaf (see "File Storage").
To alter or add an xmlSplitter file, adjust the filtering "if" statement:

** filtering statement **

---------------------
Webpage Maintenance

The PoliScrape webpage is a PHP hierarchy, hosted at http://poliscrape-rsteed11.apps.unc.edu. The PHP hierarchy code can be found in full in the GitHub repository in the <PoliScrape/publicWww/> directory. It is composed of four main components: a home page, database tab, GitHub source links, and the PoliScrape documentation. The browsing tab iterates through data housed in the <PoliScrape/publicWww/Database/> directory, creating links for subdirectories and displaying XML levels. The Database tab is intended to display an HTML version of the PoliScrape stored data hierarchy, though it is not linked directly to Longleaf due to storage restrictions.
The webpage is constructed from the Bootstrap Heroic Features template using Font Awesome fonts, found here:

** Bootstrap Heroic Features **
** FontAwesome **

An nginx-driven PHP localhost was used for development. The PoliScrape-config.php file defines base URI paths for both development environment and production environment. The production environment does not share the same root pathway. Production is maintained on Carolina Cloud Apps through a GitHub submodule. Currently, it is only available for modification by Ryan Steed, the account owner. A GitHub contributor may publish changes as merge requests, which can be fetched and merged into the production submodule and published to production. Refer to the GitHub documentation for more information on submodules and version control.

** GitHub version control again **

Searching

PoliScrape searching is unavailable in Version 1.

Use Read the Docs Sphinx theme.












