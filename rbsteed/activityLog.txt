	---------------- PoliScrape Activity Log ------------------
------------------- Ryan Steed -----------

Link to research journal: https://docs.google.com/document/d/13azVS2LlGunDKLAi91zUEvBUVs-EYhM4Ns51-xilf5g/edit.


8-25

TODO:

//- Find book IBN//
//- Look into, take notes on web crawling - try to start something?//
//- Visit Undergrad Library to look into textbook//

LEARNED: 
- Google works so fast because it saves search results on server (not possible for us)
- Jumping from site to site is relatively easy, but figuring out how to extract information from FRUS files is tougher
- How to parse results so understandable?

8-30

TODO:

//- Adapted search terms, returns text around search and formats text better. -//
//- Trawled through Wisconsin, identified simple terms and brought back sample text. -//


9-1

TODO:

//- Doesn't recognize some links?//
- How to deal with PDFs: http://digicoll.library.wisc.edu/cgi-bin/FRUS/FRUS-idx?type=article&did=FRUS.FRUS1862v01.i0002&id=FRUS.FRUS1862v01&isize=M 
//- Robots.txt?//
- Work from html.

LEARNED:

- No PID... non-onyen Sakai login?
- At Wisconsin, PDFs are scans - no text to analyze, but for the title.
- All three websites used for testing do not have robots.txt files.
- To do this from HTML
	* Use php to exec(some unix command)
	* Get result -> a file -> the html page

9-6

TODO:

- For Dr. McKeown
	* Demo web crawler proto - features:
		- Visits web pages.
		- Checks page status - 200 visited and available, 404 not found
		- Searches for search term provided by converting page text to string, lowercase.
		- If not found, collects searchable URLs from page and adds them to "toVisit" list - visits until term is found
		- Stops at defined limit.
		- Provides sample of text around search term, once found.
	* Features to add:
		- Visit robots.txt file and use that to search -- can't happen, none of the target links have a sitemap.
		- Compile all search results and don't stop at first (harder)
		- Put the search results in a more useful and comprehensible form - provide link to source.
		- Deal with visual PDFs...
	* Biggest challenges: 
		- Refining search so that the crawler doesn't waste time - Office of the Historian great because all html, but bad because of lots of links!
		- i.e., refine search locations - would be useful to have sitemap
		- Saving a variety of searches in a server, to reduce search times
		- Learn to use php within webpages - should be useful to have iwphys development environment

LEARNED: 

- Madison contact, when ready
- Look through web crawler information to become more efficient
- Deal with PDFs
	* theory: used to be in two formats, PDF and text
	* use test strings that get small number of hits - contained in paper
	* Error analysis
- Crack FRUS (economically), incrementally add target sites 
- Solve Onyen problem, Sakai connection

9-8

TODO:

- Investigate Lemur and similar text-parsing search tools
- Look for proxy services that will expedite coding process
- Record sources, current methodologies for Mentorship assignment

LEARNED:

- Installed lemur library, quite the process
	- Lemur parses XML, HTML, PDF, etc. and streamlines search engines
	- Would need to use Web Parser
	- Not sure how this works when connecting to websites... seems more like you have a file and search it from terminal?
- Scrapy
	- In python
	- Added new example crawler

9-13 

TODO:

- Look into getting that textbook
- Investigate results of Scrapy web crawler
- Figure out what's going on in Sakai...
- Investigate other proxy services at bookmarked page

LEARNED:

- Building working Scrapy proto

9-15

TODO:

- Use Scrapyd to manage crawler
- Investigate other proxies
//- Textbook form//
- Can't seem to get anything to print to page...
				
LEARNED:

- Stuff is printing now!
- New prototype that can scrape body paragraph - investigating ways to parse that data
- Progress on Onyen

9-20

TODO:

//- For Dr. McKeown:
	- Looked at lemur project - can be used for text analysis.
	- Open source web crawler scrapy:
		- Framework for web searching
		- Uses Python (a new language, but hasn't been hard to figure out)	
		- Check out all those imports - this is why Scrapy is good
		- Can print body text, just like in old prototype - fairly easy
		- Set out rules to follow links on page - a bit more difficult, but much more customizable!!
		- Can have multiple start urls and limit to a certain domain
		- Also follows general bot rules - has manners
		
		- More efficient because it parses out specific data using tag collectors, items - faster, baby
		- Scrapyd allows scheduling of crawls - will basically print to a directory which can be analyzed using Lemur, or something of the sort

	- In conclusion: continue with "scraping" style, or back away and take the slower (hopeless) approach.

	- Send Dr. McKeown the GitHub repo
	- Onyen stuff?
	- What next?
		- Text analysis or further scraping?
		- Some of the context we discussed - materials to look at
		- Sakai course materials - what to focus on

	- Notes:
		- Unclassified classification manual. First 30-40 pages.
		- NSA has best discussions of administrative politics.
		- Sylvan and Majeski papers - more technical details than the textbook.
		- Dr. McKeown might have some more stuff
			- check his citations
			- Benoit, Crimmer - technically advanced, inconsequential
		- Political science papers are mostly proof of concept - not as oriented towards whether useful for theoretical questions//


- How much space? Research.
- Send GitHub repository.

LEARNED:

	- Dr. McKeown will give me more stuff.
	- We might need more space.

9-22

TODO:

	- How much space? Research.
	- Add copies of external resources to actual project for use... aagh
	//- Follow scraped urls - callback.//
	- Get rid of extra stuff in result.
	- Tie to html.

LEARNED:

	- Explanation of Modules, Classes and Objects - instantation: https://learnpythonthehardway.org/book/ex40.html
	- Crawler now produces two files: links found and body text scraped.
	- Some directory format changes.
	- Crawled another absolute URL, need to collect links now.

9-27

TODO:
	
	//- Read declassification materials first - write up next research article?//
	- Refine that url callback, baby - is it working properly? Are all searchables searched?
	- Reconcile Scrapy parsing and other parsing.
	- Tie to html.
	- Get rid of extra stuff in result.

LEARNED: 

	- On the wi-fi!
	- Declassification notes (https://www.fas.org/sgp/othergov/intel/capco_reg_v6-0.pdf, pg. 28-38)
		- Top Secret, Secret, Confidential, and Unclassified classification markings
		- Originators of intelligence assign the classification marking
		- RELIDO if without any guidance
		- other markings for foreign release (NOFORN, Explicit Disclosure and Release)
		- EO 13526 says info that could cause exceptionally grave damage to national security (defined by the original classification authority, OCA) is TOP Secret
		- Secret can be mingled with other classification levels within the same document; info that could cause "serious" damage
		- Confidential could reasonably cause damage to national security
		- Unclassified is everything else
		- REL TO USA, FVEY means good for Australia, Canada, New Zealand and UK too
		- Special mark for releasable to UK even if secret
		- Another for Japan
		- Why only specific countries?
		- the joint, NATO, non-US, non-intelligence, and “sensitive  compartmented information” classifications that makes up the bulk of the volume will be less than 1% of declassified material, so a quick skim of that material is enough
	- Gov't Secrecy (https://fas.org/issues/government-secrecy/)
		- Federation of American Scientists works to promote public access

	- Now print Scrapy stuff to a log
	- Bug where only a set number of files download
	- Attempt to use extensions to limit number of pages visited, or time spent

9-29

TODO:
	
	//- Sylvan and Majeski article.//
	//- Get scrapy to stop scraping.//
	- Get rid of unnecessary pages?
	- Tie to html.

LEARNED: 

	- Fixed wierd bug where only ten are printed - using pageCount instead of timeout for now.
	- Adjusted file naming to avoid IOError
	- Now running until timeout, replaces / with - and removes http and // to make file name
	- for timeout=30, produces 

	- Sylvan and Majeski "How Foreign Policy Recommendations Are Put Together: A Computational Model with Empirical Applications"
		- computational model of how recommendations by U.S. policy makers are assembled in foreign policy
		- policy is the connection of certain strings of words to other strings, explains foreign policy phenomena
		- can model these strings with programming!
		- the meaning of foreing policy acts (dropping bombs) can still be described by the words of high-level officials; activities are mostly verbal
		- culture is problem-solving (sometimes a front for personal reasons), argumentative (oral and written arguments to persuade others), imperial in nature (client states, biggest problems for US are the ones client states can't solve and we have to step in on)
		- consider modes of connecting things
		- association of inputs yields outputs
		- used archives heavily because they want to account for details of recommendations - want to predict kind of escalation advocated and reasons, not just that there is escalation suggested
		- recommendations have specific goals; different "lines of policy" compete and the one that wins is the recommendation
		- "tools" pursue goals and "missions" are the concrete things done; scale up and down these components when comparing solutions
		- two groups of problems: those with current policy, and those that would arise if other policy were used
		- Summary of Problems:
			- current (an argument against using other policy)
				- current proximate goal
					- instrument
					- action
					- location
				- mission
				- reasons for current failure to accomplish goal
				- assessment
					- pure failure
					- partial or improper implementation
					- will eventually succeed
		- The Model
			- November 1961 debate over deployment to South Vietnam; Taylor and McNamara argued for major commitment south of 17th parallel
			- generally good matches can be obtained for 51 other policy lines
			- Works fairly well, kind of a stretch though
		- Takeaways:
			- Interesting argument-based model
			- Surprisingly good results, but very limited - not a prophet
			- interesting that such limiting definitions can produce decent results (tools, etc.)
			- application of this research? certainly seems modern

10/4

TODO: 
	
	- For Dr. McKeown
		- Demonstrate Scrapy crawler results, explain further steps for parsing and specific tools added; URL callback is done and results are stored in JSON
		- Next step is to remove all this extraneous text (one day job) and then send it on an extended crawl (set up a schedule to run over a weekend?)
		- Go over article briefly, wi-fi problem solved, book coming in

	- Read Holloway article
	- Do the other html pages
	- Set up test heirarchy
	- Get rid of unnecessary pages?
	- Tie to html with php file.
		- before design text analysis, assess searching capabilities through PHP

LEARNED

	- Where do proximate goals archives come from in the first place? derivation of goal structure isn't derived, but is rather implicit
	- Book is designed for general, non-technical audience; not as advanced as AI ambitions in article
	- Foreign policy prophet seems to be the end goal, but is particularly ambitious
	- Activity in the area is most likely not publicly available - conference papers appearing, batted around, vanishing (poster sessions that don't translate into papers)
	- Sylvan and Majeski reassembly of strings reflect foreign policy decision-making, not highly constrained; who conducts activity and the subjects are very constrained
		- high level decisions are constrained to around 5 people; things on the agenda can't be resolved at lower levels; certain communications are irrelevant to decision-making in this model
		- decision-making system is closed in this case; informational input (text strings) but the processing is narrow and based solely on small group's reading of the situation
	- Wisconsin people

	- "Leading the Conversation: Comparing State Department Communication Networks under Rogers and Kissinger", James Hollway and Jon Mellon
		- Database of diplomatic communication within State Department from NARA diplomatic cables
		- Used a 'novel citation typology' to analyze influence Secretary has over conversations emerging from networks of inter-cited messages
		- Does info come from the top or the bottom?
		- who begins, manages, and ends conversations such that they are influencing the flow of information in the State Department?
		- organizational chart of state dep.
		- diplomatic cables still used much the same (since 1973!)
		- Reasoning for bottom-up
			- risk reduction
			- cost reduction
			- epistemic communities: expertise in their domains
			- coalition building
		- Top down
			- information/processing advantages: more dedicated analysts
			- principal-agent concerns: need to maintain control over embassies and avoid skewed reports
			- heirarchy: clear lines of authority
			- Nixon's distrust of state department
		- graphed communications network; graphed typical communications structures




