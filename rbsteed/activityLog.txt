---------------- PoliScrape Activity Log ------------------
------------------- Ryan Steed -----------

Link to research journal: https://docs.google.com/document/d/13azVS2LlGunDKLAi91zUEvBUVs-EYhM4Ns51-xilf5g/edit.


8-25

TODO:

//- Find book IBN//
//- Look into, take notes on web crawling - try to start something?//
//- Visit Undergrad Library to look into textbook//

LEARNED: 
- Google works so fast because it saves search results on server (not possible for us)
- Jumping from site to site is relatively easy, but figuring out how to extract information from FRUS files is tougher
- How to parse results so understandable?

8-30

TODO:

//- Adapted search terms, returns text around search and formats text better. -//
//- Trawled through Wisconsin, identified simple terms and brought back sample text. -//


9-1

TODO:

//- Doesn't recognize some links?//
- How to deal with PDFs: http://digicoll.library.wisc.edu/cgi-bin/FRUS/FRUS-idx?type=article&did=FRUS.FRUS1862v01.i0002&id=FRUS.FRUS1862v01&isize=M 
//- Robots.txt?//
- Work from html.

LEARNED:

- No PID... non-onyen Sakai login?
- At Wisconsin, PDFs are scans - no text to analyze, but for the title.
- All three websites used for testing do not have robots.txt files.
- To do this from HTML
	* Use php to exec(some unix command)
	* Get result -> a file -> the html page

9-6

TODO:

- For Dr. McKeown
	* Demo web crawler proto - features:
		- Visits web pages.
		- Checks page status - 200 visited and available, 404 not found
		- Searches for search term provided by converting page text to string, lowercase.
		- If not found, collects searchable URLs from page and adds them to "toVisit" list - visits until term is found
		- Stops at defined limit.
		- Provides sample of text around search term, once found.
	* Features to add:
		- Visit robots.txt file and use that to search -- can't happen, none of the target links have a sitemap.
		- Compile all search results and don't stop at first (harder)
		- Put the search results in a more useful and comprehensible form - provide link to source.
		- Deal with visual PDFs...
	* Biggest challenges: 
		- Refining search so that the crawler doesn't waste time - Office of the Historian great because all html, but bad because of lots of links!
		- i.e., refine search locations - would be useful to have sitemap
		- Saving a variety of searches in a server, to reduce search times
		- Learn to use php within webpages - should be useful to have iwphys development environment

LEARNED: 

- Madison contact, when ready
- Look through web crawler information to become more efficient
- Deal with PDFs
	* theory: used to be in two formats, PDF and text
	* use test strings that get small number of hits - contained in paper
	* Error analysis
- Crack FRUS (economically), incrementally add target sites 
- Solve Onyen problem, Sakai connection

9-8

TODO:

- Investigate Lemur and similar text-parsing search tools
- Look for proxy services that will expedite coding process
- Record sources, current methodologies for Mentorship assignment

LEARNED:

- Installed lemur library, quite the process
	- Lemur parses XML, HTML, PDF, etc. and streamlines search engines
	- Would need to use Web Parser
	- Not sure how this works when connecting to websites... seems more like you have a file and search it from terminal?
- Scrapy
	- In python
	- Added new example crawler

9-13 

TODO:

- Look into getting that textbook
- Investigate results of Scrapy web crawler
- Figure out what's going on in Sakai...
- Investigate other proxy services at bookmarked page

LEARNED:

- Building working Scrapy proto

9-15

TODO:

- Use Scrapyd to manage crawler
- Investigate other proxies
//- Textbook form//
- Can't seem to get anything to print to page...
				
LEARNED:

- Stuff is printing now!
- New prototype that can scrape body paragraph - investigating ways to parse that data
- Progress on Onyen

9-20

TODO:

//- For Dr. McKeown:
	- Looked at lemur project - can be used for text analysis.
	- Open source web crawler scrapy:
		- Framework for web searching
		- Uses Python (a new language, but hasn't been hard to figure out)	
		- Check out all those imports - this is why Scrapy is good
		- Can print body text, just like in old prototype - fairly easy
		- Set out rules to follow links on page - a bit more difficult, but much more customizable!!
		- Can have multiple start urls and limit to a certain domain
		- Also follows general bot rules - has manners
		
		- More efficient because it parses out specific data using tag collectors, items - faster, baby
		- Scrapyd allows scheduling of crawls - will basically print to a directory which can be analyzed using Lemur, or something of the sort

	- In conclusion: continue with "scraping" style, or back away and take the slower (hopeless) approach.

	- Send Dr. McKeown the GitHub repo
	- Onyen stuff?
	- What next?
		- Text analysis or further scraping?
		- Some of the context we discussed - materials to look at
		- Sakai course materials - what to focus on

	- Notes:
		- Unclassified classification manual. First 30-40 pages.
		- NSA has best discussions of administrative politics.
		- Sylvan and Majeski papers - more technical details than the textbook.
		- Dr. McKeown might have some more stuff
			- check his citations
			- Benoit, Crimmer - technically advanced, inconsequential
		- Political science papers are mostly proof of concept - not as oriented towards whether useful for theoretical questions//


- How much space? Research.
- Send GitHub repository.

LEARNED:

	- Dr. McKeown will give me more stuff.
	- We might need more space.

9-22

TODO:

	- How much space? Research.
	- Add copies of external resources to actual project for use... aagh
	//- Follow scraped urls - callback.//
	- Get rid of extra stuff in result.
	- Tie to html.

LEARNED:

	- Explanation of Modules, Classes and Objects - instantation: https://learnpythonthehardway.org/book/ex40.html
	- Crawler now produces two files: links found and body text scraped.
	- Some directory format changes.
	- Crawled another absolute URL, need to collect links now.

9-27

TODO:
	
	//- Read declassification materials first - write up next research article?//
	- Refine that url callback, baby - is it working properly? Are all searchables searched?
	- Reconcile Scrapy parsing and other parsing.
	- Tie to html.
	- Get rid of extra stuff in result.

LEARNED: 

	- On the wi-fi!
	- Declassification notes (https://www.fas.org/sgp/othergov/intel/capco_reg_v6-0.pdf, pg. 28-38)
		- Top Secret, Secret, Confidential, and Unclassified classification markings
		- Originators of intelligence assign the classification marking
		- RELIDO if without any guidance
		- other markings for foreign release (NOFORN, Explicit Disclosure and Release)
		- EO 13526 says info that could cause exceptionally grave damage to national security (defined by the original classification authority, OCA) is TOP Secret
		- Secret can be mingled with other classification levels within the same document; info that could cause "serious" damage
		- Confidential could reasonably cause damage to national security
		- Unclassified is everything else
		- REL TO USA, FVEY means good for Australia, Canada, New Zealand and UK too
		- Special mark for releasable to UK even if secret
		- Another for Japan
		- Why only specific countries?
		- the joint, NATO, non-US, non-intelligence, and “sensitive  compartmented information” classifications that makes up the bulk of the volume will be less than 1% of declassified material, so a quick skim of that material is enough
	- Gov't Secrecy (https://fas.org/issues/government-secrecy/)
		- Federation of American Scientists works to promote public access

	- Now print Scrapy stuff to a log
	- Bug where only a set number of files download
	- Attempt to use extensions to limit number of pages visited, or time spent

9-29

TODO:
	
	//- Sylvan and Majeski article.//
	//- Get scrapy to stop scraping.//
	- Get rid of unnecessary pages?
	- Tie to html.

LEARNED: 

	- Fixed wierd bug where only ten are printed - using pageCount instead of timeout for now.
	- Adjusted file naming to avoid IOError
	- Now running until timeout, replaces / with - and removes http and // to make file name
	- for timeout=30, produces 

	- Sylvan and Majeski "How Foreign Policy Recommendations Are Put Together: A Computational Model with Empirical Applications"
		- computational model of how recommendations by U.S. policy makers are assembled in foreign policy
		- policy is the connection of certain strings of words to other strings, explains foreign policy phenomena
		- can model these strings with programming!
		- the meaning of foreing policy acts (dropping bombs) can still be described by the words of high-level officials; activities are mostly verbal
		- culture is problem-solving (sometimes a front for personal reasons), argumentative (oral and written arguments to persuade others), imperial in nature (client states, biggest problems for US are the ones client states can't solve and we have to step in on)
		- consider modes of connecting things
		- association of inputs yields outputs
		- used archives heavily because they want to account for details of recommendations - want to predict kind of escalation advocated and reasons, not just that there is escalation suggested
		- recommendations have specific goals; different "lines of policy" compete and the one that wins is the recommendation
		- "tools" pursue goals and "missions" are the concrete things done; scale up and down these components when comparing solutions
		- two groups of problems: those with current policy, and those that would arise if other policy were used
		- Summary of Problems:
			- current (an argument against using other policy)
				- current proximate goal
					- instrument
					- action
					- location
				- mission
				- reasons for current failure to accomplish goal
				- assessment
					- pure failure
					- partial or improper implementation
					- will eventually succeed
		- The Model
			- November 1961 debate over deployment to South Vietnam; Taylor and McNamara argued for major commitment south of 17th parallel
			- generally good matches can be obtained for 51 other policy lines
			- Works fairly well, kind of a stretch though
		- Takeaways:
			- Interesting argument-based model
			- Surprisingly good results, but very limited - not a prophet
			- interesting that such limiting definitions can produce decent results (tools, etc.)
			- application of this research? certainly seems modern

10/4

TODO: 
	
	- For Dr. McKeown
		- Demonstrate Scrapy crawler results, explain further steps for parsing and specific tools added; URL callback is done and results are stored in JSON
		- Next step is to remove all this extraneous text (one day job) and then send it on an extended crawl (set up a schedule to run over a weekend?)
		- Go over article briefly, wi-fi problem solved, book coming in

	- Read Holloway article
	- Do the other html pages
	- Set up test heirarchy
	- Get rid of unnecessary pages?
	- Tie to html with php file.
		- before design text analysis, assess searching capabilities through PHP

LEARNED:

	- Where do proximate goals archives come from in the first place? derivation of goal structure isn't derived, but is rather implicit
	- Book is designed for general, non-technical audience; not as advanced as AI ambitions in article
	- Foreign policy prophet seems to be the end goqal, but is particularly ambitious
	- Activity in the area is most likely not publicly available - conference papers appearing, batted around, vanishing (poster sessions that don't translate into papers)
	- Sylvan and Majeski reassembly of strings reflect foreign policy decision-making, not highly constrained; who conducts activity and the subjects are very constrained
		- high level decisions are constrained to around 5 people; things on the agenda can't be resolved at lower levels; certain communications are irrelevant to decision-making in this model
		- decision-making system is closed in this case; informational input (text strings) but the processing is narrow and based solely on small group's reading of the situation
	- Wisconsin people

	- "Leading the Conversation: Comparing State Department Communication Networks under Rogers and Kissinger", James Hollway and Jon Mellon
		- Database of diplomatic communication within State Department from NARA diplomatic cables
		- Used a 'novel citation typology' to analyze influence Secretary has over conversations emerging from networks of inter-cited messages
		- Does info come from the top or the bottom?
		- who begins, manages, and ends conversations such that they are influencing the flow of information in the State Department?
		- organizational chart of state dep.
		- diplomatic cables still used much the same (since 1973!)
		- Reasoning for bottom-up
			- risk reduction
			- cost reduction
			- epistemic communities: expertise in their domains
			- coalition building
		- Top down
			- information/processing advantages: more dedicated analysts
			- principal-agent concerns: need to maintain control over embassies and avoid skewed reports
			- heirarchy: clear lines of authority
			- Nixon's distrust of state department
		- graphed communications network; graphed typical communications structures

10/6

TODO:


	//- Try and except unicode codec error//
	- Do the other html pages
	- Set up test heirarchy
	- Get rid of unnecessary pages?
	- Tie to html with php file.
		- before design text analysis, assess searching capabilities through PHP

LEARNED:
	
	- Decided to leave in unicode, difficult to parse and should be fine when rendering
	- Scrapyd: used scrapyd client to eggify project and add to server
	- configured scrapy.cfg 
	- Use scrapyd-deploy to prepare project in egg
	- Use curl http://localhost:6800/schedule.json -d project=poliScrapy -d spider=history to schedule
	- don't forget to start scrapyd with scrapyd command
	- Can run crawlers without limitations, working on getting items to return
	- Successfully saved to server!!! Now, how to access those files later/maintain that server

10/11

TODO:
	
	- Figure out how to make best use of Scrapyd and how to access the data there from html
	//- Add in other html pages and domains//
	- Setup a test heirarchy and search itself?

LEARNED: 

	- Using bootstrap template "Heroic Features" as homepage, will feature various databases available across web with descriptions in separate pages
	- Configured php files with path URL for prod
	- Kept reading paper

10/13

TODO 
	
	- Finish formatting bootstrap HTML web layout, update GitHub project for any visitors
	- Updated README file
	- Project limitations: can we do both collection and analysis, or should we leave analysis for another time? or concentrate on analysis with straight FRUS now and worry about other stuff later?

10/18

TODO
	
	- Talk to Dr. McKeown about obtaining prod server

LEARNED

	- Started longer term run of Scrapyd
	- https://meta-toolkit.org/
	- Added tab to page with scraped data
	- https://doc.scrapy.org/en/latest/topics/item-pipeline.html
	- https://doc.scrapy.org/en/latest/topics/exporters.html

10-20

TODO

	//- Figure out how to run Scrapyd on same server as PoliScrape directory - will matter later in prod//
	- Export items to a better format
	- Look into security issues of Scrapyd
	- Use lemur for data analysis...

LEARNED 

	- Lots of time fixing network permissions bug, kept on 6800 server because no point in moving it
	- Configuring item pipelines; still cannot achieve, will use MongoDB
	- Sylvan and Majeski (!)


10-25

TODO
	
	- Configure MongoDB pipeline and set up Mongo DB to recieve data!
	- Look into security issues of Scrapyd
	- Use Lemur for data analysis...

LEARNED

	- For Dr. McKeown
		- PoliScrape
			- Using bootstrap template "Heroic Features" as homepage, will feature various databases available across web with descriptions in separate pages
				- Has first tab, which can feature different databases and various features of the site (search techniques)
				- Has database tab, which currently has a series of concatenated docs - will transition to MongoDB shortly
				- Has GitHub feature, which gives statistics, issues, etc.
				- Other features you would like to see?
				- Marketing? Name for project?
			- Configured php files with path URL for prod
			- Started longer term run of Scrapyd - once on prod, can let run overnight/weeks
			- Let's look into getting production... (licensing? space?)
			- Second half of trimester will be data analysis, it would seem
			- Some issues to resolve:
				- how do I test full saturation - time to scrape is a limiting factor
				- use Lemur or similar in basic tests - configure, essentially
				- Presenting files usefully - edit PHP
		- I have a book! What to read...
		- Discuss Hollway
		- My Mentorship Schedule??

		- Talk to IT support services, write up request; talk to someone in person
		- File names with page names in tree hierarchy
		- Paper
			- theory of decision-making close to the data and to actual decision-making (not microeconomic logic)
			- historically grounded approach
			- "combines man and machine" - series of interrogatives

10/27

TODO
	- Talk to IT support services, write up request; talk to someone in person
	- File names with page names in tree hierarchy

LEARNED
	- IT says go through the PoliSci dept.
	- Tried to get data.php to render database as series of links
	- Problems with .jl files not wanting to display, need to use php include but gets commented out in transfer process - create template?

11/8

TODO
	- Problems with .jl files not wanting to display, need to use php include but gets commented out in transfer process - create template?
	- File names with page names in tree hierarchy
	- IT?

LEARNED
	- Moved tasks to https://trello.com/b/deLTrYZ7/poliscrape-2016; further logs will include only "LEARNED" section
	- "Decision Making by Objection and the Cuban Missile Crisis," by Paul A. Anderson
		- Analysis of missile crisis documents questions standard definition of a "decision"
			- Involved sequential choice over array of noncompeting courses of action
			- Decision-making led to discovery of goals, not the other way around
			- More concerned with avoiding failure than achieving success
			- Calls it "objection"
		- conventional:
			- identify goals
			- search for alternate courses
			- predict consequences of each
			- evaluate in terms of goal
			- select best alternative for achieving the goal
			- requires cycles
		- categorized interactions in discussion in the ExCom by assigning codes to particular statements
		- sets up a series of questions for user to answer about the text
		- don't advocate alternatives, just object to them
		- yes-no choices for a bunch of different alternatives
		- satisfactory alternative often has low probability of success

11-15 

LEARNED
	- things that matter
		- type of document
		- date created
		- organization or agency producing
		- author or authors, if known
	- how to search by date - reference existing pages
	- Iraq War
		- papers on public opinion, how agency performance is evaluated
		- Effect of Cold War end on intelligence
		- Iraq 2001-2003: Read Sakai chapters of Polar (2), Polluck (1)
		- Talk about it, then get a sense for what to read next

11-29

LEARNED
	- Pillar Ch. 2
		- no policy process in decision to start Iraq war
		- Had to sell the idea of invasion; hyped terrorism and WMDs
		- Pinned any error on misinformation, so based on the presesnce of WMDs
		- Desert Storm reversed Kuwait invasion, but didn't depose; some in Bush (II) admin wanted to finish business
		- supported by neoconservatives (open letters with signatures like Rumsfeld's)
		- neoconservatives believed freedom values must be spread
		- this was primary motivation, though not used early on
		- WHAT EVIDENCE DO WE HAVE THEY DIDN'T JUST MAKE THIS UP AFTER? AUTHOR DOESN'T PROVIDE SIGNIFICANT SUPPORT
		- Iraq beginning of capitalizing on single superpower status
		- helped that Saddam was an abhorrent dictator
		- oil means invasion might be cheaper; also gave Iraq disproportionate power in the region
		- cost to Israel was 0, they were more likely to favor; benefited them by distracting and justifying own military occupation; invasion would rebalance region in Israel's favor
		- some officials sympathetic to Israel, but wasn't the primary motivation
		- W. Bush started off immediately focused on Iraq
		- tone was, "get rid of Hussein and all problems are solved, so how can we do it"
		- invasion most certain way to do it, best way to show what U.S. policy is all about
		- 9/11 a chance for all out offensive
		- Iraq would be much easier than Afghanistan; couldn't go immediately, would be obviously contrived
		- Bush told Rumsfeld to make new plan for invasion after Taliban driven from Kabul, secretive
		- State of the Union in 2002 combined terrorism, WMDs, 'axis of evil'
		- State department started to take notice, reservations quashed!
		- 'SELLING OF THE WAR'
		- spent a year priming public for invasion, not out of the blue
		- SEEEMS TO CONTRAST WITH CONTINUOUS, FLOW OF INFORMATION VIEW OF DECISION-MAKING; MORE DETERMINISTIC, ADMINSTRATION-BASED
		- Thinks Bush, admin decided to wage war in first weeks after 9/11, spent 1.5 years priming public opinion
		- Intelligence did not drive decision, but was used in arguments
		- In fact, briefings before inauguration barely mentioned Iraq
		- Intelligence community judged for getting things wrong, and for resisting politicians
		- Somehow ignored issue that Saddam wouldn't use potential WMD without a provocation, like a freaking war!

1 Dec 2016

LEARNED: 

	Subsequently, I researched the VCL, a virtual programming system that can install Scrapy and the necessary Python packages. This should be sufficient for our project as neither program requires a DBMS (database management service), only pure Python code and a few Python packages. Lemur is in Java and requires a few Linux libraries. We are working with research computing to set up the proper UNC environment to handle the program and begin prototype testing. 

6 Dec 2016

LEARNED:
	
	- met with Sandeep!
		- Decided to go with VCL - has 10 GB of storage soft cap, use vcl.unc.edu to make a reservation
		- Sandeep will get image editing permissions so I can create my own disk image to use (software)
		- Create reservation, ssh into VCL and work until time limit expires; work will disappear!
		- will push any results to GitHub when finished
		- also creating a Mass Storage account to store info; haven't decided yet the way that the storage will be handled
		- how to get Lemur to work? might work on KillDevil/Longleaf, definitely would work on the VCL (Virtual Computing Log)
		- recommended Carolina Cloud for web hosting
			- application: 
				URL:        http://poliscrape-rsteed11.apps.unc.edu/
  				SSH to:     58471a9a42d9f5d4fd00006d@poliscrape-rsteed11.apps.unc.edu
  				Git remote: ssh://58471a9a42d9f5d4fd00006d@poliscrape-rsteed11.apps.unc.edu/~/git/poliscrape.git/
  				Cloned to:  /Users/Ryan/poliProd/poliscrape
  			- no success adding files to Carolina github that aren't in the root directory of hte project, oddly

 8 Dec 2016

 - logged into Longleaf succesfully, inititated storage structure with GitHub repo
 - logged into VCL session with Ubuntu
 - still don't have VCL disk image permissions, oddly; contacted Sandeep
 - cleaned up repo and worked on website publishing

 13 Dec 2016

 - Information Retrieval Web Scraping reading
 - Holsti chapters on public opinion and congress - role of those bodies in the Iraq War
 
 - Disk Imaging process
 	- working inside virtualenv, in folder in machine which I activated with "bin/activate"
 	- downloaded pip package, virtualenv and Scrapy, and BeautifulSoup
 	- Called Scrapy, Ubuntu 14.04 LTS Svr (Full Blade)

 12 Jan 2017

 - Some Notes from "Introduction to Information Retrieval," by Manning, Raghavan and Schutze: see Dropbox

 19 Jan 2017
 
 created submodule in poliprod dir, links to PoliScrape main website
 now published at http://poliscrape-rsteed11.apps.unc.edu/PoliScrape/publicWww/
 git fetch and git merge to update changes, from PoliScrape dir

 scp myfile.txt rsteed11@longleaf.its.unc.edu:/ms/depts/polisci/LemurProj